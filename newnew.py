import requests
from bs4 import BeautifulSoup
import json
import os

# ç¢ºä¿ taiwan_regions è³‡æ–™å¤¾å­˜åœ¨
os.makedirs("taiwan_regions", exist_ok=True)

# å­˜å„²æ‰€æœ‰åœ°å€çš„æ•¸æ“š
all_region_data = []

# é€²å…¥å°ç£æ—…éŠç¶²ç«™çš„é¦–é 
response = requests.get("https://www.taiwan.net.tw/")
soup = BeautifulSoup(response.text, "html.parser")

# å–å¾—æ¯å€‹å€åŸŸï¼ˆä¾‹å¦‚åŒ—éƒ¨åœ°å€ã€ä¸­éƒ¨åœ°å€...ï¼‰
viewpoints = soup.find_all("a", class_="megamenu-btn")
for viewpoint in viewpoints:
    region = viewpoint.getText().strip()
    url = viewpoint.get("href")
    url_response = requests.get("https://www.taiwan.net.tw/" + url)
    url_soup = BeautifulSoup(url_response.text, "html.parser")

    # é€™å€‹åœ°å€çš„å€åŸŸè³‡æ–™
    region_data = {"region": region, "cities": []}

    # é€²å…¥æ¯å€‹å€åŸŸçš„è©³ç´°æ™¯é»ï¼ˆä¾‹å¦‚ï¼šå°åŒ—å¸‚ã€åŸºéš†å¸‚...ï¼‰
    de_viewpoints = url_soup.find_all("a", class_="circularbtn")
    for de_viewpoint in de_viewpoints:
        city_name = (
            de_viewpoint.find("span", class_="circularbtn-title").getText().strip()
        )

        print(f"æ­£åœ¨è™•ç†ï¼š{region} - {city_name}")

        de_url = de_viewpoint.get("href")
        de_url_response = requests.get("https://www.taiwan.net.tw/" + de_url)
        de_url_soup = BeautifulSoup(de_url_response.text, "html.parser")

        # åŸå¸‚æ•¸æ“š
        city_data = {"city": city_name, "spots": []}

        # æ‰¾åˆ°æ¯å€‹æ™¯é»çš„è©³ç´°ä¿¡æ¯
        cards = de_url_soup.find_all("div", class_="card")
        for card in cards:
            # æ™¯é»æ¨™é¡Œ
            title_tag = card.find("div", class_="card-title")
            title = title_tag.get_text(strip=True) if title_tag else "æœªçŸ¥æ™¯é»"
            print("title:", title)

            # åœ–ç‰‡æå–
            graphic_div = card.find("div", class_="graphic")
            img_tag = graphic_div.find("img") if graphic_div else None
            images = []
            if img_tag:
                img_url = img_tag.get("data-src") or img_tag.get("src")
                if img_url:
                    images.append(img_url.strip())

            # æ™¯é»çš„ hashtags
            hashtag_div = card.find("div", class_="hashtag")
            hashtags_data = []
            if hashtag_div:
                hashtag_links = hashtag_div.find_all("a")
                hashtags_data = [link.get_text(strip=True) for link in hashtag_links]

            link_tag = card.find("a")
            de_second_url = link_tag.get("href")
            de_second_response = requests.get(
                "https://www.taiwan.net.tw/" + de_second_url
            )
            de_second_soup = BeautifulSoup(de_second_response.text, "html.parser")

            content_texts = []
            wrap_div = de_second_soup.find("div", class_="content")
            wrap_div = wrap_div.find("div", class_="wrap")

            if wrap_div:
                paragraphs = wrap_div.find_all("p")
                print("paragraphs", paragraphs)
                content_texts = [p.get_text(strip=True) for p in paragraphs]

            # æ•´ç†æ¯å€‹æ™¯é»çš„æ•¸æ“š
            spot_data = {
                "title": title,
                "images": images,
                "hashtags": hashtags_data,
                "contents": content_texts,
            }
            city_data["spots"].append(spot_data)

        # å°‡åŸå¸‚æ•¸æ“šæ·»åŠ åˆ°åœ°å€
        region_data["cities"].append(city_data)

    all_region_data.append(region_data)
    print(f"âœ… {region} çš„æ•¸æ“šå·²æˆåŠŸè™•ç†")

# å°‡æ‰€æœ‰åœ°å€çš„æ•¸æ“šå­˜å„²ç‚ºçµ±ä¸€çš„ JSON æ–‡ä»¶
all_region_filename = "taiwan_regions/taiwan_all_regions.json"
with open(all_region_filename, "w", encoding="utf-8") as f:
    json.dump(all_region_data, f, ensure_ascii=False, indent=4)

print(f"ğŸ‰ æ‰€æœ‰åœ°å€çš„æ•¸æ“šå·²å„²å­˜åœ¨ {all_region_filename}")
